{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_w-7ZF9WmWC"
      },
      "source": [
        "# Quelques exemples de fonctions d'activation de base\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kd01IKzdWmWD"
      },
      "source": [
        "### Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ShyJUe2VWmWE"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "x = np.arange(-6, 6, 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_RTo7D7WmWE"
      },
      "source": [
        "### Linear  :  Fonction d'activation linéaire\n",
        "\n",
        "C'est une fonction simple de la forme: f(x) = ax ou f(x) = x. En gros, l'entrée passe à la sortie sans une très grande modification ou alors sans aucune modification. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yoxBCvBSWmWF"
      },
      "outputs": [],
      "source": [
        "def linear(x):\n",
        "    a = []\n",
        "    for item in x:\n",
        "        a.append(item)\n",
        "    return a\n",
        "\n",
        "y = linear(x)\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd_1He_kWmWG"
      },
      "source": [
        "### Sigmoid\n",
        "\n",
        "En mathématiques, la fonction sigmoïde (dite aussi courbe en S) est définie par :\n",
        "\n",
        "$$ f(x)=\\frac{1}{1 + e^{- x}}$$ pour tout réel x ;\n",
        "\n",
        "\n",
        "Le but premier de la fonction est de réduire la valeur d'entrée pour la réduire entre 0 et 1. En plus d'exprimer la valeur sous forme de probabilité, si la valeur en entrée est un très grand nombre positif, la fonction convertira cette valeur en une probabilité de 1. A l'inverse, si la valeur en entrée est un très grand nombre négatif, la fonction convertira cette valeur en une probabilité de 0. D'autre part, l'équation de la courbe est telle que, seules les petites valeurs influent réellement sur la variation des valeurs en sortie.\n",
        "\n",
        "La fonction Sigmoïde a plusieurs défaults:\n",
        "\n",
        "- Elle n'est pas centrée sur zéro, c'est à dire que des entrées négatives peuvent engendrer des sorties positives.\n",
        "\n",
        "- Etant assez plate, elle influe assez faiblement sur les neurones par rapport à d'autres fonctions d'activations. Le résultat est souvent très proche de 0 ou de 1 causant la saturation de certains neurones.\n",
        "\n",
        "- Elle est couteuse en terme de calcul car elle comprend la fonction exponentielle.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "XiQyZuPMWmWG"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "    a = []\n",
        "    for item in x:\n",
        "        a.append(1/( (1+math.exp(-item) * 1))) \n",
        "        # ici j'ai pri A = 1 : plus A est grand plus on se rapproche à la fonction echelon ...\n",
        "        \n",
        "    return a\n",
        "\n",
        "y = sigmoid(x)\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6pM0PdtWmWH"
      },
      "source": [
        "### Tahn :  Tangente Hyperbolique \n",
        "\n",
        "Cette fonction ressemble à la fonction Sigmoïde. La différence avec la fonction Sigmoïde est que la fonction Tanh produit un résultat compris entre -1 et 1. La fonction Tanh est en terme général préférable à la fonction Sigmoïde car elle est centrée sur zéro. Les grandes entrées négatives tendent vers -1 et les grandes entrées positives tendent vers 1.\n",
        "\n",
        "Mis à part cet avantage, la fonction Tanh possède les mêmes autres inconvénients que la fonction Sigmoïde."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lL-Po6KzWmWH"
      },
      "outputs": [],
      "source": [
        "def tanh(x, derivative=False):\n",
        "    if (derivative == True):\n",
        "        return (1 - (x ** 2))\n",
        "    return np.tanh(x)\n",
        "\n",
        "\n",
        "y = tanh(x)\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su5qkZlWWmWI"
      },
      "source": [
        "### ReLU : Unité de Rectification Linéaire\n",
        "Pour résoudre le problème de saturation des deux fonctions précédentes (Sigmoïde et Tanh) il existe la fonction ReLU (Unité de Rectification Linéaire). Cette fonction est la plus utilisée.\n",
        "\n",
        "La fonction ReLU est inteprétée par la formule: f(x) = max(0, x). Si l'entrée est négative la sortie est 0 et si elle est positive, alors la sortie est égale à x. Cette fonction d'activation augmente considérablement la convergence du réseau et ne sature pas.\n",
        "\n",
        "Mais la fonction ReLU n'est pas parfaite. Si la valeur d'entrée est négative, le neurone reste inactif, ainsi les poids ne sont pas mis à jour et le réseau n’apprend pas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NaHM-WDRWmWI"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    a = []\n",
        "    for item in x:\n",
        "        if item > 0:\n",
        "            a.append(item)\n",
        "        else:\n",
        "            a.append(0)\n",
        "    return a\n",
        "\n",
        "\n",
        "y = relu(x)\n",
        "\n",
        "plt.plot(x,y)\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpQRZSswWmWI"
      },
      "source": [
        "## Construire un modèle de réseaux de neurones "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nO3TA6f7WmWJ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "print (tf.__version__)\n",
        "keras.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eYo87D2WmWJ"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2eLH6ACWmWJ"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "#(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "# the data, shuffled and split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odgDUCJSWmWJ"
      },
      "outputs": [],
      "source": [
        "x_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOXq4OkDWmWK"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8cNaXhPsWmWK"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vJ-_e5bQWmWK"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.reshape(60000, 784) #  28*28\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RGjh84XWmWK"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U9XLxq-BWmWK"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Declaration du modèle en Tensorflow2.0\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(tf.keras.layers.Dense(10, activation='sigmoid', input_dim =(784)))\n",
        "model.add(tf.keras.layers.Dense(10, activation='sigmoid'))\n",
        "model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "# résumé du modèle\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32wR8ClWWmWL"
      },
      "outputs": [],
      "source": [
        "batch_size = 100\n",
        "#num_classes = 10\n",
        "epochs= 50\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',  optimizer='SGD',  metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jng3P9z1WmWL"
      },
      "outputs": [],
      "source": [
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1,verbose=1 )\n",
        "\n",
        "#verbose: Integer. 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
        "# Je vous invite à lire la documentation : https://keras.io/models/sequential/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qTbNPwg6WmWL"
      },
      "outputs": [],
      "source": [
        "test_loss, test_acc = model.evaluate(x_test, y_test,verbose=0)\n",
        "\n",
        "print('Test loss:', test_loss)\n",
        "print('Test accuracy:', test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G4jaLW6WmWL"
      },
      "source": [
        "## Exercice : \n",
        "On a obtenu avec ce modèle basique, un taux d'accuracy égale à 84,64%.\n",
        "- Essayer d'améliorer la performence du modèle, en modifiant les fonctions d'activation, ou/et en n ajoutant le nombre de neurones et des couches intermédiaires.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u_TTk0YGWmWL"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "TP1_Some_basics_about_learning_process.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}